{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Statistics \n",
    "\n",
    "Yi-Chi Liao (yi-chi.liao@aalto.fi) \n",
    "\n",
    "PhD student in User Interfaces group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of the lecture<a id=\"outline\">\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2><img src=\"../images/goal.png\" width=20, align=\"left\">&nbsp;After the lecture, you should know...</h2>\n",
    "    <ul>\n",
    "        <li>Frequentists vs Bayesians (<a href=\"#0\">Part 0</a>):\n",
    "            <ul>\n",
    "              <li><a href=\"#01\">what is probability</a></li>\n",
    "              <li><a href=\"#02\">frequentist stats analysis</a></li>\n",
    "              <li><a href=\"#03\">Bayesian stats analysis</a></li>\n",
    "              <li><a href=\"#04\">basic concept of data generation</a></li>\n",
    "            </ul>\n",
    "            <br>\n",
    "        <li>Generating data using probability (<a href=\"#1\">Part 1</a>):\n",
    "            <ul>\n",
    "              <li><a href=\"#11\">simulating data with uniform distribution</a></li>\n",
    "              <li><a href=\"#12\">binomial distibution</a></li>\n",
    "              <li><a href=\"#13\">ECDFs</a></li>\n",
    "              <li><a href=\"#14\">Poison distribution</a></li>\n",
    "              <li><a href=\"#15\">exponential distribution</a></li>\n",
    "              <li><a href=\"#16\">normal distribution</a></li>\n",
    "            </ul>\n",
    "            <br>\n",
    "        <li>Bayes' Theorem and Bayesian inference (<a href=\"#2\">Part 2</a>)\n",
    "            <ul>\n",
    "              <li><a href=\"#21\">conditional and joint probability</a></li>\n",
    "              <li><a href=\"#22\">Bayes' Theorem</a></li>\n",
    "              <li><a href=\"#23\">Bayesian Inference</a></li>\n",
    "            </ul>\n",
    "            <br>\n",
    "        <li>Bayesian stastistics (<a href=\"#3\">Part 3</a>)\n",
    "            <ul>\n",
    "              <li><a href=\"#31\">basic steps of Bayesian stats analysis</a></li>\n",
    "              <li><a href=\"#32\">Parameter estimation</a></li>\n",
    "              <li><a href=\"#33\">comparing A/B group</a></li>\n",
    "            </ul>\n",
    "            <br>\n",
    "        <li><a href=\"#summary\">Summary</a>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "from empiricaldist import Pmf\n",
    "from collections import Counter\n",
    "from ipywidgets import interact\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Frequentist vs Bayesian<a id=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2><img src=\"../images/goal.png\" width=20, align=\"left\">&nbsp;Learning objectives of part 0</h2>\n",
    "    <br>Learn to:\n",
    "    <ul>\n",
    "        <li>know the foundamental difference between frequentists' and Bayesians' view of probability\n",
    "        <li>know the credibility region of Bayesian probability\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Probability<a id=\"01\">\n",
    "\n",
    "Discussion: What is probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bayesian inference differs from more traditional statistical inference by preserving *uncertainty*. At first, this sounds like a bad statistical technique. Isn't statistics all about deriving *certainty* from randomness? To reconcile this, we need to start thinking like Bayesians. \n",
    "\n",
    "> The Bayesian world-view interprets probability as measure of *believability in an event*, that is, how confident we are in an event occurring. In fact, we will see in a moment that this is the natural interpretation of probability. \n",
    "\n",
    "> For this to be clearer, we consider an alternative interpretation of probability: *Frequentist*, known as the more *classical* version of statistics, assumes that probability is the long-run frequency of events (hence the bestowed title). For example, the *probability of plane accidents* under a frequentist philosophy is interpreted as the *long-term frequency of plane accidents*. This makes logical sense for many probabilities of events, but becomes more difficult to understand when events have no long-term frequency of occurrences. Consider: we often assign probabilities to outcomes of presidential elections, but the election itself only happens once! Frequentists get around this by invoking alternative realities and saying across all these realities, the frequency of occurrences defines the probability. \n",
    "\n",
    "> Bayesians, on the other hand, have a more intuitive approach. Bayesians interpret a probability as measure of *belief*, or confidence, of an event occurring. Simply, a probability is a summary of an opinion. An individual who assigns a belief of 0 to an event has no confidence that the event will occur; conversely, assigning a belief of 1 implies that the individual is absolutely certain of an event occurring. Beliefs between 0 and 1 allow for weightings of other outcomes. This definition agrees with the probability of a plane accident example, for having observed the frequency of plane accidents, an individual's belief should be equal to that frequency, excluding any outside information. Similarly, under this definition of probability being equal to beliefs, it is meaningful to speak about probabilities (beliefs) of presidential election outcomes: how confident are you candidate *A* will win?\n",
    "\n",
    "\n",
    "-- _Probabilistic Programming and Bayesian Methods for Hackers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: If you throw a coin 20 times, and get 15 heads, will you call it a fair coin?  \n",
    "Given another known fair coin, how will design an experiment to justify the previous observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Frequentist statistical analysis and p-value<a id=\"02\">\n",
    "\n",
    "Frequentists' solution: Assume the skeptic is correct (this is a fair point that has 0.5 chance produce head), what is the probability (long-run frequency of this event) of having 15 or more heads simply by chance?\n",
    "\n",
    "Flip the coin once, the chance of having a head is:  \n",
    "$p(H) = \\frac{1}{2} $  \n",
    "\n",
    "Flip the coin twice, the chance of having two head is:  \n",
    "$p(HH) = \\frac{1}{2} \\times \\frac{1}{2}$  \n",
    "\n",
    "And so on...   \n",
    "$p(HHT) = \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8}$    \n",
    "\n",
    "If we don't care about the order of appearance:  \n",
    "$p(2H, 1T) = p(HHT) + p (HTH) + p (THH)= 3 \\times \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{3}{8}$   \n",
    "\n",
    "This is known as the binomial distribution:  \n",
    "$p (N_H, N_T) ={{N}\\choose{N_H}}(\\frac{1}{2})^{N_H}(1 - \\frac{1}{2})^{N_T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binom_pmf(n=1, p=0.1, head_times=1):\n",
    "\n",
    "    binom_dis = binom(n, p)  ## A function imported from Scipy\n",
    "    x = np.arange(binom_dis.ppf(0.0001), binom_dis.ppf(0.9999))\n",
    "    binom_p = binom_dis.pmf(x)  \n",
    "    idx = 0\n",
    "    prob_more_than_t = 0\n",
    "    if (head_times in x):\n",
    "        idx = np.where(x==head_times)[0][0]\n",
    "        prob_more_than_t = np.sum(binom_p[idx:])\n",
    "        print (\"The probability of having >= \", head_times, \"times of heads is (p-value) \",prob_more_than_t)\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(x, binom_dis.pmf(x), 'bo', c='b',label='binom pmf')\n",
    "    ax.vlines(x, 0, binom_dis.pmf(x), colors='b', lw=5, alpha=0.5)\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "    ax.plot([head_times,head_times],[0,0.18],c='r')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xlabel('Times of heads')\n",
    "    plt.title('PMF of binomial calculated distribution(n={}, p={})'.format(n, p))\n",
    "    plt.show()\n",
    "\n",
    "binom_pmf(n=20, p=0.5,head_times =15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of 0.02 (2%) of observations given a fair coin **(one-tail)**.  \n",
    "Conclusion: we can reject the null hypothesis (this is a fair coin) at p < 0.05.\n",
    "\n",
    "Frequentists analyze the p-values as such: If the experiment is repeated many times, how many cases the computed confidence interval will contain the the current or even more extreme event.\n",
    "\n",
    "**Side note:**   \n",
    "PMF = Probability Mass Function (gives the probability that a discrete random variable is exactly equal to some value.)   \n",
    "PDF = Probability Density Function (specifies the probability of the random variable falling within a particular range of values, as opposed to taking on any one value.)\n",
    "\n",
    "### Hands-on \n",
    "What is the probability of having >= 12 heads given 20 coin flips? Can you reject the null hypothesis?  \n",
    "What is the probability of having >= 25 heads given 40 coin flips? Can you reject the null hypothesis now?  \n",
    "\n",
    "<a href=\"#coin_data_gen\">Jump to simulation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see this from a more general view:\n",
    "In general, a frequentist 95% Confidence Intervals is not any particular interval has 95% likelihood (or confidence, or belief) to contain the true value!\n",
    "\n",
    "If you think that, **you're actually a Bayesian!**\n",
    "\n",
    "![frequentist0](../images/frequentist0.png)\n",
    "\n",
    "It actually means that **95% of the samples in repeated experiments will form such intervals that contain the true value**.  \n",
    "\n",
    "If p-value < 0.05, that means given the ture value, there are only less than 5% of such samples contain our particular observation (frequency of heads) or even more extreme cases.\n",
    "![frequtist](../images/frequentist4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jackie](../images/jackie.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A typical conversation:\n",
    "\n",
    "Statistician: \"95% of such confidence intervals in repeated measurments will contain the true value.\"  \n",
    "Scientist/Engineer: \"So there is 95% of chance that the value is in the interval.\"  \n",
    "\n",
    "Statistician: \"No, you see, parameters (true values) by definition can't vary, so referring to chance in that context is meaningless. The 95% chance refers to the confidence interval itself.\"  \n",
    "Scientist/Engineer: \"Oh! So there is 95% of chance that the value is in the interval?\"   \n",
    "\n",
    "Statistician: \"No. It's like this: the long-term limiting frequency of the procedure of constructing this interval ensures that 95% of the resulting ensemble of intervals will contain the value.\"  \n",
    "Scientist/Engineer: \"Ah! I see! So there is 95% of chance that the value is in the interval, right?\"   \n",
    "\n",
    "Statistician: \"No... It's that... Well... just write down what I said, okay?\"  \n",
    "Scientist/Engineer: \"Ok! Got it! The true value is 95% likely to be in the interval.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still have questions, it's highly recommended to view this excellent blog artical: [frequentist vs Bayesian](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Frequentists' view\n",
    "\n",
    "0. (Unintuitive way of seeing probability)\n",
    "1. The setting of p-value and number of samples are problematic in practice.\n",
    "2. It can only tell if there is a difference, or an effect. There is no information when the outcome is insignificant.\n",
    "3. It does not provide the model based on our current obsevations. For example, given the observations of 15 heads and 5 tails, it does not tell you the probability of 14 heads and 6 tails if you flip the coin another 20 times.\n",
    "4. You need to use various methods for different tests!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Bayesian statistical analysis and credibility region<a id=\"03\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_euro(data, hypo):\n",
    "    x = hypo / 100\n",
    "    if data == 'H':\n",
    "        return x\n",
    "    else:\n",
    "        return 1-x\n",
    "\n",
    "def decorate_euro(title):\n",
    "    plt.xlabel('Probability of heads (unit:%)')\n",
    "    plt.ylabel('PMF')\n",
    "    plt.title(title)\n",
    "\n",
    "euro = Pmf.from_seq(range(101))\n",
    "euro.plot()\n",
    "decorate_euro('Prior distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro.update(likelihood_euro, 'H')\n",
    "euro.plot()\n",
    "decorate_euro('Posterior distribution, one heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro.update(likelihood_euro, 'T')\n",
    "euro.plot()\n",
    "decorate_euro('Posterior distribution, one heads one tails')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro.update(likelihood_euro, 'H')\n",
    "euro.plot()\n",
    "decorate_euro('Posterior distribution, two heads one tails')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro = Pmf.from_seq(range(101))\n",
    "head_times = 15\n",
    "tail_times = 5\n",
    "evidence = 'H' * head_times + 'T' * tail_times\n",
    "for outcome in evidence:\n",
    "    euro.update(likelihood_euro, outcome)\n",
    "    \n",
    "euro.plot()\n",
    "decorate_euro('Posterior distribution, '+str(head_times) +' heads, ' + str(tail_times)+ ' tails')\n",
    "cred_interval = euro.credible_interval(0.95)\n",
    "print (\"The 95% credibility interval is from \", cred_interval[0], \"% to \", cred_interval[1], \"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on \n",
    "What is the distribution if you observe 13 heads and 7 tails? Is 50% within the credibility interval now?  \n",
    "What is the distribution if you observe 26 heads and 14 tails? \n",
    "\n",
    "Bayesians' view: We can create a model (distribution of belief) based on the observations, then check if our hypothesis is in the credibility region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayesian](../images/bayesian2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Bayesians probability\n",
    "1. Bayesian provides a more natural interpretation of the data.\n",
    "2. Bayesian provides an informative model based on the current samples.\n",
    "3. It does not simply tell you if there is an effect, but also tell you how strong the effect is.\n",
    "4. Bayesian analysis draws more reasonable conclusions from small-sample studies.\n",
    "5. There is only one method -- simulation and inference -- in Bayesian analysis, while traditional statistics has A LOT OF methods for different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Bayesian probability\n",
    "\n",
    "1. Probabilistic modelling and simulation  \n",
    "2. Bayesian inference\n",
    "3. Bayesian stastistics\n",
    "4. Gaussian Process and Bayesian Optimization (not covered today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Coin-flipping with data generation<a id=\"04\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binom_simulation(n=1, p=0.1, sample=10000, head_times=1):\n",
    "    binom_data = np.random.binomial(n, p, size=sample)\n",
    "    bino_s_distribution = np.zeros((n))\n",
    "    for i in range(len(bino_s_distribution)):\n",
    "        bino_s_distribution[i] = np.count_nonzero(binom_data ==i)/sample\n",
    "    \n",
    "    x = np.arange(n)\n",
    "    prob_more_than_t = np.sum(bino_s_distribution[head_times:])\n",
    "    print (\"The probability of having >= \", head_times, \"times of heads is \", round(prob_more_than_t,8))\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(x, bino_s_distribution, 'bo', c='b',label='binom simulated pmf')\n",
    "    ax.vlines(x, 0,bino_s_distribution, colors='b', lw=5, alpha=0.5)\n",
    "    ax.legend(loc='upper left', frameon=False)\n",
    "    ax.plot([head_times,head_times],[0,0.18],c='r')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xlabel('Times of heads')\n",
    "    plt.title('PMF of binomial simulated distribution(n={}, p={})'.format(n, p))\n",
    "    plt.show()\n",
    "\n",
    "binom_simulation(n=20, p=0.5,sample=20,head_times =15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the distribution same as the calculated binomial distribution? Why not?\n",
    "\n",
    "### Hands-on <a id=\"coin_data_gen\"> \n",
    "- What is the probability if you give it 100 samples? \n",
    "- What is the probability given 1,000,000 samples? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binom_pmf(n=20, p=0.5,head_times =15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#outline\">Going back to outline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Part 1: Probability Simulation<a id=\"1\">\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2><img src=\"../images/goal.png\" width=20, align=\"left\">&nbsp;Learning objectives of part 1</h2>\n",
    "    <br>Learn to:\n",
    "    <ul>\n",
    "        <li>understand of what \"probability\" means, in both Bayesian and Frequentist terms\n",
    "        <li>simulate probability distributions that model real-world phenomena\n",
    "        <li>have a basic idea of classic distributions\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 1.1 Simulating probabilities<a id=\"11\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's say 50% of the Aalto students have iPhone. If we picked 1000 people at random from the population, how likely would it be to find that a certain number of them having iPhone?\n",
    "\n",
    "We can simulate this using `numpy`'s random number generator.\n",
    "\n",
    "To do so, first note we can use `np.random.rand()` to randomly select floats between 0 and 1 (known as the _uniform distribution_). Below, we do so and plot a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 1,000 samples from uniform & plot results\n",
    "x = ___\n",
    "plt.hist(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To then simulate the sampling from the population, we check whether each float was greater or less than 0.5. If less than or equal to 0.5, we say the person has an iPhone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computed how many people click\n",
    "iphone = ____\n",
    "n_iphone = sum(iphone)\n",
    "f\"Number of iphone = {n_iphone}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of people who own iphone can be calculated as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computed proportion of people with an iphone\n",
    "f\"Proportion who owns iphone = {n_iphone/len(iphone)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: Did you get the same answer as your neighbor? If you did, why? If not, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Up for discussion:** Let's say that all you had was this data and you wanted to figure out the iPhone rate (probability of a student in Aalto has iPhone). \n",
    "\n",
    "* What would your estimate be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Although, in the above, we have described _probability_ in two ways, we have not described it mathematically. We're not going to do so rigorously here, but we will say that _probability_ defines a function from the space of confidence (in the above, the interval $[0,1]$) that describes how likely it is to get a particular point or region in that space. Mike Betancourt has an elegant [Introduction to Probability Theory (For Scientists and Engineers)](https://betanalpha.github.io/assets/case_studies/probability_theory.html) that I can recommend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on: iPhone example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use random sampling to simulate how many people have iPhone when the iPhone rate is 0.7. How many people in total? What proportion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "iphone = ____\n",
    "n_iphone = sum(iphone)\n",
    "print(f\"Number of iphone = {n_iphone}\")\n",
    "print(f\"Proportion who owns iphone = {n_iphone/len(iphone)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Discussion point_: This model is known as the bias coin flip. \n",
    "- Can you see why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perofrmance of a 3D pointing task in virtual reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate such proportions with real-world data. Here we import a dataset of 3D pointing experiment that was collected in our recently submission.\n",
    "\n",
    "\n",
    "\n",
    "![selection](../images/app-3dtouch-device.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and view head of data\n",
    "df_12 = pd.read_csv('../data/user1_raw_log.csv')\n",
    "df_12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_time = df_12['completion_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the proportion of completion time  > 1200 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ____\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion: A proxy for probability\n",
    "\n",
    "As stated above, we have calculated a frequency as probability. As a proxy for the probability, we can further simulate drawing random samples (with replacement) from the data seeing how many trials have completion time are > 1200 and calculating the proportion (commonly referred to as [hacker statistics](https://speakerdeck.com/jakevdp/statistics-for-hackers)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "sum(np.random.choice(c_time, n_samples, replace=True) > 1200)/n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember\n",
    "\n",
    "We just experienced a simple example that we built an extremely simple probabilistic model with a small dataset. The probabilistic model essntially is a function describes how likely it is to get a particular point or region in a defined space. Upon this model, we then generated a bunch of simulated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Another way to simulate coin-flips (Binomial distribution)<a id=\"12\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, you have used the uniform distribution to sample from a series of biased coin flips. I want to introduce you to another distribution that you can also use to do so: the **binomial distribution**.\n",
    "\n",
    "The **binomial distribution** with parameters $n$ and $p$ is defined as the probability distribution of\n",
    "\n",
    "> the **number of heads seen** when flipping a coin $n$ times when  with $p(heads)=p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that this distribution essentially tells the **story** of a general model in the following sense: if we believe that they underlying process generating the observed data has a binary outcome (affected by disease or not, head or not, 0 or 1, clicked through or not), and that one the of the two outcomes occurs with probability $p$, then the probability of seeing a particular outcome is given by the **binomial distribution** with parameters $n$ and $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any process that matches the coin flip story is a Binomial process (note that you'll see such coin flips also referred to as Bernoulli trials in the literature).  So we can also formulate the story of the Binomial distribution as\n",
    "\n",
    "> the number $r$ of successes in $n$ Bernoulli trials with probability $p$ of success, is Binomially distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use the binomial distribution to answer the same question as above:\n",
    "* If P(heads) = 0.7 and you flip the coin ten times, how many heads will come up?\n",
    "\n",
    "We'll also set the seed to ensure reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(seed=16071982)\n",
    "\n",
    "# Simulate one run of flipping the biased coin 10 times\n",
    "np.random.binomial(____, ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating many times to get the distribution\n",
    "\n",
    "In the above, we have simulated the scenario once. But this only tells us one potential outcome. To see how likely it is to get $n$ heads, for example, we need to simulate it a lot of times and check what proportion ended up with $n$ heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate 1,000 run of flipping the biased coin 10 times\n",
    "x = np.random.binomial(____, ____, ____)\n",
    "\n",
    "# Plot normalized histogram of results\n",
    "plt.hist(x, density=True, bins=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Group chat: what do you see in the above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on: Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If I flip a biased coin ($P(H)=0.3$) 20 times, what is the probability of 5 or more heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.random.binomial(____) >= 5)/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If I flip a fair coin 20 times, what is the probability of 5 or more heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.random.binomial(____) >= 5)/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the normalized histogram of number of heads of the following experiment: flipping a fair coin 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram \n",
    "x = np.random.binomial(10, 0.5, 10000)\n",
    "plt.hist(x, density=True, bins=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** you may have noticed that the _binomial distribution_ can take on only  a finite number of values, whereas the _uniform distribution_ above can take on any number between $0$ and $1$. These are different enough cases to warrant special mention of this & two different names: the former is called a _probability mass function_ (PMF) and the latter a _probability distribution function_ (PDF). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What is the probability of having more than 4 heads? Can you easily tell that from a histogram plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Empirical cumulative distribution functions (ECDFs)<a id=\"13\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ECDF is, as an alternative to a histogram, a way to visualize univariate data that is rich in information. It allows you to visualize all of your data and, by doing so, avoids the very real problem of binning.\n",
    "- can plot control plus experiment\n",
    "- data plus model!\n",
    "- many populations\n",
    "- can see multimodality (though less pronounced) -- a mode becomes a point of inflexion!\n",
    "- can read off so much: e.g. percentiles.\n",
    "\n",
    "See Eric Ma's post on ECDFS [here](https://ericmjl.github.io/blog/2018/7/14/ecdfs/) AND [this twitter thread](https://twitter.com/allendowney/status/1019171696572583936).\n",
    "\n",
    "So what is  this ECDF? \n",
    "\n",
    "**Definition:** In an ECDF, the x-axis is the range of possible values for the data & for any given x-value, the corresponding y-value is the proportion of data points less than or equal to that x-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a handy ECDF function that takes in data and outputs $x$ and $y$ data for the ECDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    # Number of data points\n",
    "    n = len(data)\n",
    "\n",
    "    # x-data for the ECDF\n",
    "    x = np.sort(data)\n",
    "\n",
    "    # y-data for the ECDF\n",
    "    y = np.arange(1, n+1) / n\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on: Plotting ECDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ECDF for the previous hands-on  exercise. Read the answer to the following question off the ECDF: he probability of seeing 4 or more heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x- and y-data for the ECDF\n",
    "x_flips, y_flips = ____\n",
    "\n",
    "# Plot the ECDF\n",
    "plt.plot(x_flips, y_flips, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability distributions and their stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we saw that we could match data-generating processes with binary outcomes to the story of the binomial distribution.\n",
    "\n",
    "> The Binomial distribution's story is as follows: the number $r$ of successes in $n$ Bernoulli trials with probability $p$ of success, is Binomially distributed. \n",
    "\n",
    "There are many other distributions with stories also!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Poisson processes and the Poisson distribution<a id=\"14\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the book [Information Theory, Inference and Learning Algorithms](https://www.amazon.com/Information-Theory-Inference-Learning-Algorithms/dp/0521642981) David MacKay tells the tale of a town called Poissonville, in which the buses have an odd schedule. Standing at a bus stop in Poissonville, the amount of time you have to wait for a bus is totally independent of when the previous bus arrived. This means you could watch a bus drive off and another arrive almost instantaneously, or you could be waiting for hours.\n",
    "\n",
    "Arrival of buses in Poissonville is what we call a Poisson process. The timing of the next event is completely independent of when the previous event happened. Many real-life processes behave in this way. \n",
    "\n",
    "* natural births in a given hospital (there is a well-defined average number of natural births per year, and the timing of one birth is independent of the timing of the previous one);\n",
    "* Landings on a website;\n",
    "* Meteor strikes;\n",
    "* Molecular collisions in a gas;\n",
    "* Aviation incidents.\n",
    "\n",
    "Any process that matches the buses in Poissonville **story** is a Poisson process.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of arrivals of a Poisson process in a given amount of time is Poisson distributed. The Poisson distribution has one parameter, the average number of arrivals in a given length of time. So, to match the story, we could consider the number of hits on a website in an hour with an average of six hits per hour. This is Poisson distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Poisson-distributed data\n",
    "samples  =  np.random.poisson(6, size=10**6)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(samples, bins=21);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Does this look like anything to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the Poisson distribution is the limit of the Binomial distribution for low probability of success and large number of trials, that is, for rare events. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this, think about the stories. Picture this: you're doing a Bernoulli trial once a  minute for an hour, each with a success probability of 0.05. We would do 60 trials, and the number of successes is Binomially distributed, and we would expect to get about 3 successes. This is just like the Poisson story of seeing 3 buses on average arrive in a given interval of time. Thus the Poisson distribution with arrival rate equal to np approximates a Binomial distribution for n Bernoulli trials with probability p of success (with n large and p small). This is useful because the Poisson distribution can be simpler to work with as it has only one parameter instead of two for the Binomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands-on: Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ECDF of the Poisson-distributed data that you generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x- and y-data for the ECDF\n",
    "x_p, y_p = ecdf(samples)\n",
    "\n",
    "# Plot the ECDF\n",
    "plt.plot(x_p, y_p, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Poisson distribution: field goals attempted per game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first remind ourselves of the story behind the Poisson distribution.\n",
    "> The number of arrivals of a Poisson processes in a given set time interval is Poisson distributed.\n",
    "\n",
    "We could model field goal attempts in a basketball game using a Poisson distribution. When a player takes a shot is a largely stochastic process, being influenced by the myriad ebbs and flows of a basketball game. Some players shoot more than others, though, so there is a well-defined rate of shooting. Let's consider LeBron James's field goal attempts for the 2017-2018 NBA season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing's first, the data ([from here](https://www.basketball-reference.com/players/j/jamesle01/gamelog/2018)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fga = [19, 16, 15, 20, 20, 11, 15, 22, 34, 17, 20, 24, 14, 14, \n",
    "       24, 26, 14, 17, 20, 23, 16, 11, 22, 15, 18, 22, 23, 13, \n",
    "       18, 15, 23, 22, 23, 18, 17, 22, 17, 15, 23, 8, 16, 25, \n",
    "       18, 16, 17, 23, 17, 15, 20, 21, 10, 17, 22, 20, 20, 23, \n",
    "       17, 18, 16, 25, 25, 24, 19, 17, 25, 20, 20, 14, 25, 26, \n",
    "       29, 19, 16, 19, 18, 26, 24, 21, 14, 20, 29, 16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that this LeBron's attempts are ~ Poisson distributed, you're now going to plot the ECDF and compare it with the the ECDF of the Poisson distribution that has the mean of the data (technically, this is the maximum likelihood estimate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands-on: Simulating Data Generating Stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the x and y values for the ECDF of LeBron's field attempt goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x & y data for ECDF\n",
    "x_ecdf, y_ecdf = ecdf(fga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll draw samples out of a Poisson distribution to get the theoretical ECDF, plot it with the ECDF of the data and see how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times we simulate the model\n",
    "n_reps = 1000\n",
    "\n",
    "# Plot ECDF of data\n",
    "plt.plot(x_ecdf, y_ecdf, '.', color='black');\n",
    "\n",
    "# Plot ECDF of model\n",
    "for _ in range(n_reps):\n",
    "    samples = np.random.poisson(np.mean(fga), size=len(fga))\n",
    "    x_theor, y_theor = ecdf(samples)\n",
    "    plt.plot(x_theor, y_theor, '.', alpha=0.01, color='lightgray');\n",
    "\n",
    "\n",
    "# Label your axes\n",
    "plt.xlabel('field goal attempts')\n",
    "plt.ylabel('ECDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the ECDF that LeBron's field goal attempts per game are Poisson distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Exponential distribution<a id=\"15\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've encountered a variety of named _discrete distributions_. There are also named _continuous distributions_, such as the Exponential distribution and the Normal (or Gaussian) distribution. To see what the story of the Exponential distribution is, let's return to Poissonville, in which the number of buses that will arrive per hour are Poisson distributed.\n",
    "However, the waiting time between arrivals of a Poisson process are exponentially distributed.\n",
    "\n",
    "So: the exponential distribution has the following story: the waiting time between arrivals of a Poisson process are exponentially distributed. It has a single parameter, the mean waiting time. This distribution is not peaked, as we can see from its PDF.\n",
    "\n",
    "For an illustrative example, lets check out the time between all incidents involving nuclear power since 1974. It's a reasonable first approximation to expect incidents to be well-modeled by a Poisson process, which means the timing of one incident is independent of all others. If this is the case, the time between incidents should be Exponentially distributed.\n",
    "\n",
    "\n",
    "To see if this story is credible, we can plot the ECDF of the data with the CDF that we'd get from an exponential distribution with the sole parameter, the mean, given by the mean inter-incident time of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nuclear power accidents data & create array of inter-incident times\n",
    "df = pd.read_csv('../data/nuclear_power_accidents.csv')\n",
    "df.Date = pd.to_datetime(df.Date)\n",
    "df = df[df.Date >= pd.to_datetime('1974-01-01')]\n",
    "inter_times = np.diff(np.sort(df.Date)).astype(float) / 1e9 / 3600 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and sample from exponential\n",
    "mean = np.mean(inter_times)\n",
    "samples = np.random.exponential(mean, size=10**6)\n",
    "\n",
    "# Compute ECDFs for sample & model\n",
    "x, y = ecdf(inter_times)\n",
    "x_theor, y_theor = ecdf(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample & model ECDFs\n",
    "plt.plot(x_theor, y_theor);\n",
    "plt.plot(x, y, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is close to being Exponentially distributed, which means that we can model the nuclear incidents as a Poisson process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Normal distribution<a id=\"16\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Normal distribution, also known as the Gaussian or Bell Curve, appears everywhere. There are many reasons for this. One is the following:\n",
    "\n",
    "> When doing repeated measurements, we expect them to be Normally distributed, owing to the many subprocesses that contribute to a measurement. This is because (a formulation of the Central Limit Theorem) **any quantity that emerges as the sum of a large number of subprocesses tends to be Normally distributed** provided none of the subprocesses is very broadly distributed.\n",
    "\n",
    "Now it's time to see if this holds for the measurements of the speed of light in the famous Michelson–Morley experiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I'll plot the histogram with a Gaussian curve fitted to it. Even if that looks good, though, that could be due to binning bias. SO then you'll plot the ECDF of the data and the CDF of the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, plot histogram \n",
    "import scipy.stats as st\n",
    "df = pd.read_csv('../data/michelson_speed_of_light.csv')\n",
    "df = df.rename(columns={'velocity of light in air (km/s)': 'c'})\n",
    "c = df.c.values\n",
    "x_s = np.linspace(299.6, 300.1, 400) * 1000\n",
    "plt.plot(x_s, st.norm.pdf(x_s, c.mean(), c.std(ddof=1)))\n",
    "plt.hist(c, bins=9, density=True)\n",
    "plt.xlabel('speed of light (km/s)')\n",
    "plt.ylabel('PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we again import a dataset of 3D selection experiment.\n",
    "\n",
    "![selection](../images/app-3dtouch-device.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/user1_raw_log.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'completion_time': 'c'})\n",
    "c = df.c.values\n",
    "x_s = np.linspace(300, 2500, 400)\n",
    "plt.plot(x_s, st.norm.pdf(x_s, c.mean(), c.std(ddof=1)))\n",
    "plt.hist(c, bins=25, density=True)\n",
    "plt.xlabel('Completion time')\n",
    "plt.ylabel('PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why it's not normally distributed? Because it's not repeated measurments of the same task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands-on: Simulating Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get speed of light measurement + mean & standard deviation\n",
    "michelson_speed_of_light = df.c.values\n",
    "mean = np.mean(michelson_speed_of_light)\n",
    "std = np.std(michelson_speed_of_light, ddof=1)\n",
    "\n",
    "# Generate normal samples w/ mean,  std of data\n",
    "samples = np.random.normal(mean, std, size=10000)\n",
    "\n",
    "# Generate data ECDF & model CDF\n",
    "x, y = ecdf(michelson_speed_of_light)\n",
    "x_theor, y_theor = ecdf(samples)\n",
    "\n",
    "# Plot data & model (E)CDFs\n",
    "_ = plt.plot(x_theor, y_theor)\n",
    "_ = plt.plot(x, y, marker='.', linestyle='none')\n",
    "_ = plt.xlabel('speed of light (km/s)')\n",
    "_ = plt.ylabel('CDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional reading:\n",
    "\n",
    "One important thing we did not mention here is how do we sample a sequence of pointsfrom a probability distribution?  \n",
    "The answer is an algorithm called **Metropolis–Hastings algorithm**, which is an algorithm under the **Markov Chain Monte Carlo (MCMC)** methods. The most important quality of such Metropolis-Hastings algorithm is that if we make enough sample, the resulting samples will converge to the original probability distribution.\n",
    "\n",
    "You can read more from here: https://people.duke.edu/~ccc14/sta-663/MCMC.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2><img src=\"../images/warning.png\" width=\"20\", align=\"left\">&nbsp;Warning</h2>\n",
    "    <br>Make sure you are a Bayesian now: <br>\n",
    "    You (at least partially) agree that probability is credibility points assigned over a region that we can use for generating data.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#outline\">Going back to outline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Part 2: Bayes Theorem and Bayesian Inference<a id=\"2\">\n",
    "\n",
    "We have established a solid understanding of _probability_, and we learned how to create synthetic data from a probability distribution. We are about to see the other side of Bayesian probability, that is, inferring the probability distribution based on our sampled observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2><img src=\"../images/goal.png\" width=20, align=\"left\">&nbsp;Learning objectives of part 2</h2>\n",
    "    <br>Learn to:\n",
    "    <ul>\n",
    "        <li>know joint probabilities and conditional probabilities\n",
    "        <li>understand Bayes' Theorem and its utility\n",
    "        <li>understand what priors, likelihoods and posteriors are\n",
    "        <li>Use random sampling for parameter estimation to appreciate the relationship between sample size & the posterior distribution, along with the effect of the prior.\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Conditional and  Joint Probability <a id=\"21\">\n",
    "\n",
    "### Joint Probability\n",
    "\n",
    "$P(A,B)$ is the probability two events $A$ and $B$ _both_ occurring.\n",
    "* For example, getting two heads in a row.\n",
    "\n",
    "If $A$ and $B$ are independent, then $P(A,B)=P(A)P(B)$ but be warned: this is not always (or often) the case.\n",
    "\n",
    "One way to think of this is considering \"AND\" as multiplication: the probability of A **and** B is the probability of A **multiplied** by the probability of B (if they are independent!).\n",
    "\n",
    "#### Hands-On: Joint Probability and Coin Flipping\n",
    "\n",
    "Verify that $P(A,B)=P(A)P(B)$ in the two fair coin-flip case (A=heads, B=heads) by \n",
    "- first simulating two coins being flipped together and calculating the proportion of occurences with two heads;\n",
    "- then simulating one coin flip and calculating the proportion of heads and then doing that again and multiplying the two proportions.\n",
    "\n",
    "Your two calculations should give \"pretty close\" results and not the same results due to the (in)accuracy of simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate P(A,B)\n",
    "x_0 = np.random.binomial(2, 0.5, 10000)\n",
    "p_ab = ____\n",
    "\n",
    "# Now, plot the histogram of the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(x_0)\n",
    "ax.set_xticks([0,1,2])\n",
    "print(p_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate P(A)P(B)\n",
    "x_1 = ____\n",
    "x_2 = ____\n",
    "p_a = ____\n",
    "p_b = ____\n",
    "p_a*p_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and view head of data\n",
    "df_12 = pd.read_csv('../data/user1_raw_log.csv')\n",
    "df_12.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability of completion_time > 1200 -- p(A)?  \n",
    "What is the probability of spatial_error > 10 -- p(B)?\n",
    " \n",
    "If the two events are independent, what will be the joint probability (calculate p(A) x p(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p(A)\n",
    "p_ct = sum(df_12.completion_time ____)/len(df_12)\n",
    "print ('The probability of completion time > 1200 =', p_ct)\n",
    "\n",
    "# Calculate p(B)\n",
    "p_se = sum(df_12.spatial_errors ____)/len(df_12)\n",
    "print ('The probability of spatial error > 10 =', p_se)\n",
    "print ()\n",
    "\n",
    "### If A and B are independent, then P(A,B)=P(A)P(B)\n",
    "### but be warned: this is not always (or often) the case.\n",
    "print (\"If independent, the joint probability is = \", p_ct * p_se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the actual joint probability of p(A,B)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate actual p(a,b)\n",
    "df_c_time_12 = df_12.loc[df_12['completion_time'] ____]\n",
    "p_a_n_b = sum((df_c_time_12.spatial_errors ____)/len(df_12))\n",
    "print (\"The actual joined probability is = \",round (p_a_n_b, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "\n",
    "Now that we have a grasp on joint probabilities, lets consider conditional probabilities, that is, the probability of some $A$, knowing that some other $B$ is true. We use the notation $P(A|B)$ to denote this.  \n",
    "\n",
    "Conditional and joint probabilites are related by the following:\n",
    "$$ P(A,B) = P(A|B)P(B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the conditional probability, p(A|B), of spatial error > 10 (A), given completion time > 1200 (B):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c_time_12 = df_12.loc[df_12['completion_time'] > 1200]\n",
    "p_s10_given_c12 = sum(df_c_time_12.spatial_errors > 10)/len(df_c_time_12)\n",
    "print (\"The probability of having spatial error > 10 given completion time > 1200 = \", p_s10_given_c12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on:  \n",
    "Calculate the joined probability of spatial error > 10 (A) and completion time > 1200 (B) using **conditional probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the joint probability by the equation\n",
    "p_joint = round(p_s10_given_c12 * ____ ,6)\n",
    "print (\"If we use the conditional probability to calculate the joint probability, we get \",  p_joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Bayes Theorem <a id=\"22\">\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayes](../images/bayes_theo2.png)\n",
    "\n",
    "### The cookie problem\n",
    "There are two bowls of cookies: \n",
    "- Bowl #1 has 10 chocolate and 30 vanilla\n",
    "- Bowl #2 has 20 chocolate and 20 vanilla\n",
    "\n",
    "You pick a bowl at random, and pick a cookie at random,\n",
    "the cookie turns out to be vanilla. \n",
    "\n",
    "\n",
    "What is the probability that you pick Bowl #1?\n",
    "\n",
    "`Pmf.from_seq` makes a `Pmf` object from a sequence of values.\n",
    "\n",
    "Here's how we can use it to create a `Pmf` with two equally likely hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie = Pmf.from_seq(['Bowl 1', 'Bowl 2'])\n",
    "cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie['Bowl 1'] *= ____\n",
    "cookie['Bowl 2'] *= ____\n",
    "cookie.normalize()\n",
    "cookie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on: \n",
    "    \n",
    "Suppose we put the first cookie back, stir, choose again from the same bowl, and get a chocolate cookie.  \n",
    "What are the posterior probabilities after the second cookie?\n",
    "\n",
    "Hint: The posterior (after the first cookie) becomes the prior (before the second cookie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie = Pmf.from_seq(['Bowl 1', 'Bowl 2'])\n",
    "cookie['Bowl 1'] *= ____\n",
    "cookie['Bowl 2'] *= ____\n",
    "cookie.normalize()\n",
    "cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_cookie(title):\n",
    "    \"\"\"Labels the axes.\n",
    "    \n",
    "    title: string\n",
    "    \"\"\"\n",
    "    plt.xlabel('Outcome')\n",
    "    plt.ylabel('PMF')\n",
    "    plt.title(title)\n",
    "\n",
    "cookie.bar()\n",
    "decorate_cookie('Outcome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2><img src=\"../images/warning.png\" width=\"20\", align=\"left\">&nbsp;Warning</h2>\n",
    "    <br>You should have a clear idea what is Bayes Theorem and what is Bayesian update! Important concepts include: Prior (the hypothesis without data), Likelihood (given the hypothesis, what is the chance to see the data), Posterior (the updated hypothesis after seeing data).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Pmfs\n",
    "\n",
    "Let's get a bit more familiar with the Pmf package by creating a Pmf object to represent a six-sided die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d6 = Pmf()\n",
    "\n",
    "# A Pmf is a map from possible outcomes to their probabilities.\n",
    "# Let's create a fair dice\n",
    "\n",
    "for x in [1,2,3,4,5,6]:\n",
    "    d6[x] = 1\n",
    "    \n",
    "d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`normalize` adds up the probabilities and divides through.  The return value is the total probability before normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d6.normalize()\n",
    "\n",
    "d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And we can compute its mean (which only works if it's normalized).\n",
    "\n",
    "print (d6.mean())\n",
    "\n",
    "\n",
    "## choice(size=n) chooses a random values from the Pmf.\n",
    "\n",
    "print (d6.choice(size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bar` plots the Pmf as a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_dice(title):\n",
    "    \"\"\"Labels the axes.\n",
    "    \n",
    "    title: string\n",
    "    \"\"\"\n",
    "    plt.xlabel('Outcome')\n",
    "    plt.ylabel('PMF')\n",
    "    plt.title(title)\n",
    "    \n",
    "d6.bar()\n",
    "decorate_dice('One die')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d6.add_dist(d6)` creates a new `Pmf` that represents the sum of two six-sided dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twice = d6.add_dist(d6)\n",
    "twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third = twice.add_dist(d6)\n",
    "third"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on:\n",
    "Plot the distribution of twice and calculate the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twice.bar()\n",
    "decorate_dice('Two dice')\n",
    "twice.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dice problem\n",
    "\n",
    "Create a suite of hypotheses that represents dice with different numbers of sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice 4 has 4 faces, Dice 6 has 6 faces, ...\n",
    "dice = Pmf.from_seq([4, 6, 8, 12])\n",
    "dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll solve this problem two ways.  First we'll do it \"by hand\", as we did with the cookie problem; that is, we'll multiply each hypothesis by the likelihood of the data, and then renormalize.\n",
    "\n",
    "### Hands-on:\n",
    "In the space below, update `dice` based on the likelihood of the data (rolling a 6), then normalize and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice[4] *= ____\n",
    "dice[6] *= ____\n",
    "dice[8] *= ____\n",
    "dice[12] *= ____\n",
    "\n",
    "dice.normalize()\n",
    "dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on:\n",
    "\n",
    "Now let's do the same calculation using `Pmf.update`, which encodes the structure of a Bayesian update.\n",
    "\n",
    "Define a function called `likelihood_dice` that takes `data` and `hypo` and returns the probability of the data (the outcome of rolling the die) for a given hypothesis (number of sides on the die).\n",
    "\n",
    "Hint: What should you do if the outcome exceeds the hypothetical number of sides on the die?\n",
    "\n",
    "Here's an outline to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_dice(data, hypo):\n",
    "    \"\"\"Likelihood function for the dice problem.\n",
    "    \n",
    "    data: outcome of the die roll\n",
    "    hypo: number of sides\n",
    "    \n",
    "    returns: float probability\n",
    "    \"\"\"\n",
    "    # TODO: fill this in!\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create a Pmf object and update it.\n",
    "\n",
    "dice = Pmf.from_seq([4, 6, 8, 12])\n",
    "dice.update(likelihood_dice, 6)\n",
    "dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on:\n",
    "\n",
    "Create another Pmf object that update three times (got 6, 7, and 8). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "dice = Pmf.from_seq([4, 6, 8, 12])\n",
    "dice.update(____)\n",
    "dice.update(____)\n",
    "dice.update(____)\n",
    "dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we get more data, we can perform more updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = Pmf.from_seq([4, 6, 8, 12])\n",
    "for roll in [8, 7, 7, 5, 4]:\n",
    "    dice.update(likelihood_dice, roll)\n",
    "dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Euro problem\n",
    "\n",
    "*\"When spun on edge 250 times, a Belgian one-euro coin came up heads 140 times and tails 110.  'It looks very suspicious to me,' said Barry Blight, a statistics lecturer at the London School of Economics.  'If the coin were unbiased, the chance of getting a result as extreme as that would be less than 7%.' \"*\n",
    "\n",
    "From “The Guardian” quoted by MacKay, *Information Theory, Inference, and Learning Algorithms*.\n",
    "\n",
    "\n",
    "### Hands-on:\n",
    "Write a function called `likelihood_euro` that defines the likelihood function for the Euro problem.  Note that `hypo` is in the range 0 to 100.\n",
    "\n",
    "Here's an outline to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_euro(data, hypo):\n",
    "    \"\"\" Likelihood function for the Euro problem.\n",
    "    \n",
    "    data: string, either 'H' or 'T'\n",
    "    hypo: prob of heads (0-100)\n",
    "    \n",
    "    returns: float probability\n",
    "    \"\"\"\n",
    "    # TODO: fill this in!\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_euro(title):\n",
    "    \"\"\"Labels the axes.\n",
    "    \n",
    "    title: string\n",
    "    \"\"\"\n",
    "    plt.xlabel('Probability of heads')\n",
    "    plt.ylabel('PMF')\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from a uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro = Pmf.from_seq(range(101))\n",
    "euro.plot()\n",
    "decorate_euro('Prior distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can update with a single head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro.update(likelihood_euro, 'H')\n",
    "euro.plot()\n",
    "decorate_euro('Posterior distribution, one heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro.update(____)\n",
    "euro.plot()\n",
    "decorate_euro('Posterior distribution, two heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a tail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro.update(____)\n",
    "euro.plot()\n",
    "decorate_euro('Posterior distribution, HHT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting over, here's what it looks like after 4 heads and 2 tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro = Pmf.from_seq(range(101))\n",
    "\n",
    "for outcome in ____:\n",
    "    euro.update(likelihood_euro, outcome)\n",
    "\n",
    "euro.plot()\n",
    "decorate_euro('Posterior distribution, 4 heads, 2 tails')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Maximum aposteriori probability (MAP) is 67%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro.max_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro = Pmf.from_seq(range(101))\n",
    "\n",
    "evidence = ____\n",
    "for outcome in evidence:\n",
    "    euro.update(likelihood_euro, outcome)\n",
    "    \n",
    "euro.plot()\n",
    "\n",
    "decorate_euro('Posterior distribution, 140 heads, 110 tails')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior mean is about 55.92%  \n",
    "The maximum apostiori probability (MAP) is 56%  \n",
    "The medium value is also 56%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (euro.mean())\n",
    "print (euro.max_prob()) # Maximum apostiori probability\n",
    "print (euro.quantile(0.5)) # Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior credible interval has a 90% chance of containing the true value  \n",
    "(provided that the prior distribution truly represents our background knowledge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro.credible_interval(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swamping the prior\n",
    "\n",
    "The following function makes a Euro object with a triangle prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrianglePrior():\n",
    "    \"\"\"Makes a Suite with a triangular prior.\n",
    "    \"\"\"\n",
    "    suite = Pmf(name='triangle')\n",
    "    for x in range(0, 51):\n",
    "        suite[x] = x\n",
    "    for x in range(51, 101):\n",
    "        suite[x] = 100-x \n",
    "    suite.normalize()\n",
    "    return suite\n",
    "\n",
    "euro1 = Pmf.from_seq(range(101), name='uniform')\n",
    "euro1.plot()\n",
    "\n",
    "euro2 = TrianglePrior()\n",
    "euro2.plot()\n",
    "\n",
    "plt.legend()\n",
    "decorate_euro('Prior distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on\n",
    "1. Update `euro1` and `euro2` with the same data (4 heads and 2 tails) and plot the posteriors.  How big is the difference in means?\n",
    "\n",
    "2. Update `euro1` and `euro2` with the same data (40 heads and 20 tails) and plot the posteriors.  How big is the difference in means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "evidence = ____\n",
    "for outcome in evidence:\n",
    "    euro1.update(likelihood_euro, outcome)\n",
    "    euro2.update(likelihood_euro, outcome)\n",
    "\n",
    "euro1.plot()\n",
    "euro2.plot()\n",
    "\n",
    "decorate_euro('Posterior distributions')\n",
    "plt.legend()\n",
    "\n",
    "euro1.mean(), euro2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "euro1 = Pmf.from_seq(range(101), name='uniform')\n",
    "\n",
    "euro2 = TrianglePrior()\n",
    "\n",
    "\n",
    "evidence = ____\n",
    "for outcome in evidence:\n",
    "    euro1.update(likelihood_euro, outcome)\n",
    "    euro2.update(likelihood_euro, outcome)\n",
    "\n",
    "euro1.plot()\n",
    "euro2.plot()\n",
    "\n",
    "decorate_euro('Posterior distributions')\n",
    "plt.legend()\n",
    "\n",
    "euro1.mean(), euro2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 From Bayes' Theorem to (Simulated) Bayesian Inference <a id='23'>\n",
    "\n",
    "Let's say that we flip a biased coin several times and we want to estimate the probability of heads from the number of heads we saw. Statistical intuition tells us that our best estimate of $p(heads)=$ number of heads divided by total number of flips.\n",
    "\n",
    "However, \n",
    "\n",
    "1. It doesn't tell us how certain we can be of that estimate and\n",
    "2. This type of intuition doesn't extend to even slightly more complex examples.\n",
    "\n",
    "Bayesian inference helps us here. We can calculate the probability of a particular $p=p(H)$ given data $D$ by setting $A$ in Bayes Theorem equal to $p$ and $B$ equal to $D$.\n",
    "\n",
    "$$P(p|D) = \\frac{P(D|p)P(p)}{P(D)} $$\n",
    "  \n",
    "\n",
    "\n",
    "In this equation, we call $P(p)$ the prior (distribution), $P(D|p)$ the likelihood and $P(p|D)$ the posterior (distribution). The intuition behind the nomenclature is as follows: the prior is the distribution containing our knowledge about $p$ prior to the introduction of the data $D$ & the posterior is the distribution containing our knowledge about $p$ after considering the data $D$.\n",
    "\n",
    "  \n",
    "**Note** that we're _overloading_ the term _probability_ here. In fact, we have 3 distinct usages of the word:\n",
    "- The probability $p$ of seeing a head when flipping a coin;\n",
    "- The resulting binomial probability distribution $P(D|p)$ of seeing the data $D$, given $p$;\n",
    "- The prior & posterior probability distributions of $p$, encoding our _uncertainty_ about the value of $p$.\n",
    "\n",
    "**Key concept:** We only need to know the posterior distribution $P(p|D)$ up to multiplication by a constant at the moment: this is because we really only care about the values of $P(p|D)$ relative to each other – for example, what is the most likely value of $p$? To answer such questions, we only need to know what $P(p|D)$ is proportional to, as a function of $p$. Thus we don’t currently need to worry about the term $P(D)$. In fact,\n",
    "\n",
    "$$P(p|D) \\propto P(D|p)P(p) $$\n",
    "\n",
    "**Note:** What is the prior? Really, what do we know about $p$ before we see any data? Well, as it is a probability, we know that $0\\leq p \\leq1$. If we haven’t flipped any coins yet, we don’t know much else: so it seems logical that all values of $p$ within this interval are equally likely, i.e., $P(p)=1$, for $0\\leq p \\leq1$. This is known as an uninformative prior because it contains little information (there are other uninformative priors we may use in this situation, such as the Jeffreys prior, to be discussed later). People who like to hate on Bayesian inference tend to claim that the need to choose a prior makes Bayesian methods somewhat arbitrary, but as we’ll now see, if you have enough data, the likelihood dominates over the prior and the latter doesn’t matter so much.\n",
    "\n",
    "\n",
    "**Essential remark:** we get the whole distribution of $P(p|D)$, not merely a point estimate plus errors bars, such as [95% confidence intervals](http://andrewgelman.com/2018/07/04/4th-july-lets-declare-independence-95/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference using simulated data: flip those coins\n",
    "\n",
    "Now let's generate some coin flips and try to estimate $p(H)$. Two notes:\n",
    "- given data $D$ consisting of $n$ coin tosses & $k$ heads, the likelihood function is given by $L:=P(D|p) \\propto p^k(1-p)^{n-k}$;\n",
    "- given a uniform prior, the posterior is proportional to the likelihood.\n",
    "\n",
    "### Discussion: \n",
    "What is the distribution we should use for simulating data of coin flip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior(p=0.6, N=0):\n",
    "    \"\"\"Plot the posterior given a uniform prior; Bernoulli trials\n",
    "    with probability p; sample size N\"\"\"\n",
    "    # Set seed\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Flip coins \n",
    "    n_successes = np.random.binomial(N, p)\n",
    "    \n",
    "    # X-axis for PDF\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # Write out equation for uniform prior\n",
    "    prior = np.ones(len(x))\n",
    "    \n",
    "    # Write out equation for posterior, which is likelihood * prior.\n",
    "    posterior = (x**n_successes) * ((1-x)**(N-n_successes)) * prior\n",
    "    \n",
    "    # Pseudo-normalize the posterior so that we can compare them on the same scale.\n",
    "    posterior /= np.max(posterior)  \n",
    "    \n",
    "    # Plot posterior\n",
    "    plt.plot(x, posterior)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_posterior, p=(0, 1, 0.01), N=(0, 150));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you generate more and more data, your posterior gets narrower, i.e. you get more and more certain of your estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The choice of the prior\n",
    "\n",
    "You may have noticed that we needed to choose a prior and that, in the small to medium data limit, this choice can affect the posterior. We'll briefly introduce several types of priors and then you'll use one of them for the example above to see the effect of the prior:\n",
    "\n",
    "- **Informative priors** express specific, definite information about a variable, for example, if we got a coin from the mint, we may use an informative prior with a peak at $p=0.5$ and small variance. \n",
    "- **Weakly informative priors** express partial information about a variable, such as a peak at $p=0.5$ (if we have no reason to believe the coin is biased), with a larger variance.\n",
    "- **Uninformative priors** express no information about a variable, except what we know for sure, such as knowing that $0\\leq p \\leq1$.\n",
    "\n",
    "Now you may think that the _uniform distribution_ is uninformative, however, what if I am thinking about this question in terms of the probability $p$ and Jeffreys is thinking about it in terms of the _odds ratio_ $r=\\frac{p}{1-p}$? Jeffreys rightly feels that he has no prior knowledge as to what this $r$ is and thus chooses the uniform prior on $r$.\n",
    "\n",
    "With a bit of algebra (transformation of variables), we can show that choosing the uniform prior on $p$ amounts to choosing a decidedly non-uniform prior on $r$ and vice versa. So Jeffryes and I have actually chosen different priors, using the same philosophy. How do we avoid this happening? Enter the **Jeffreys prior**, which is an uninformative prior that solves this problem. You can read more about the Jeffreys prior [here](https://en.wikipedia.org/wiki/Jeffreys_prior). \n",
    "\n",
    "In the binomial (coin flip) case, the Jeffreys prior is given by $P(p) = \\frac{1}{\\sqrt{p(1-p)}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the plotting function, as above\n",
    "def plot_posteriors(p=0.6, N=0):\n",
    "    np.random.seed(42)\n",
    "    n_successes = np.random.binomial(N, p)\n",
    "    x = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "    # Write out the likelihood for the data\n",
    "    likelihood = x**n_successes*(1-x)**(N-n_successes) \n",
    "    \n",
    "    # Write out equation for posterior given uniform prior\n",
    "    prior_uniform = np.ones(len(x))\n",
    "    posterior_uniform = likelihood * prior_uniform\n",
    "    posterior_uniform /= np.max(posterior_uniform)\n",
    "    plt.plot(x, posterior_uniform, label='Uniform prior')\n",
    "    \n",
    "    # Write out equation for posterior given Jeffreys prior\n",
    "    prior_jeffreys = np.sqrt(x*(1-x))**(-1)\n",
    "    posterior_jeffreys = likelihood * prior_jeffreys\n",
    "    posterior_jeffreys /= np.max(posterior_jeffreys)\n",
    "    plt.plot(x, posterior_jeffreys, label='Jeffreys prior')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_posteriors, p=(0, 1, 0.01), N=(0, 50));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation from a model based on known user data\n",
    "\n",
    "Here we import again the dataset of 3D selection experiment that was used in our recently submitted paper. We want to learn the probability distribution of user is able to complete a selection under 1 second.\n",
    "\n",
    "![selection](../images/app-3dtouch-device.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and view head of data\n",
    "df_12 = pd.read_csv('../data/user1_raw_log.csv')\n",
    "df_12.head()\n",
    "\n",
    "# Store lengths in a pandas series\n",
    "c_time = df_12['completion_time']\n",
    "\n",
    "p = (sum(c_time < 1000))/len(c_time)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_posteriors, p=p, N=(0, 50));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2><img src=\"../images/warning.png\" width=\"20\", align=\"left\">&nbsp;Warning</h2>\n",
    "    <br>Good job! You have learned what is \"probability\" and have generated data from a distribution. Furthermore, you have a clear idea what is Bayes Theorem and its application as an inference tool.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#outline\">Going back to outline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Part 3: Bayesian Statistical Analysis with PyMC3 <a id='3'>\n",
    "\n",
    "Okay! We have a thorough understanding of Bayesian probability (part 0), the idea of data-generation with probabilistic model and MCMC sampling (part 1), and the experience of Bayesian inference (part 2). We can finally reach the last part of today's lecture -- **Bayesian statistical analysis**.\n",
    "\n",
    "![summary](../images/summary.png)\n",
    "    \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2><img src=\"../images/goal.png\" width=20, align=\"left\">&nbsp;Learning objectives of part3</h2>\n",
    "    <br>Learn to:\n",
    "    <ul>\n",
    "        <li>be familiar with the steps of doing Bayesian statistical analysis\n",
    "        <li>use basic functions of PyMC3\n",
    "        <li>do parameter estimation with PyMC3\n",
    "        <li>comparing A/B groups with PyMC3\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Basic steps of Bayesian Statistical Analysis <a id='31'>\n",
    "\n",
    "1. Collect data\n",
    "2. Parameterize your problem using statistical distributions\n",
    "3. Write model in PyMC3, and justify your model structure\n",
    "4. Hit the Inference Button\n",
    "5. Interpret based on posterior distributions\n",
    "6. (optional) with new information, modify model structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. collecting some data from users (assume we already have it)\n",
    "\n",
    "# Make the data needed for the problem.\n",
    "from random import shuffle\n",
    "total = 30\n",
    "n_heads = 11\n",
    "n_tails = total - n_heads\n",
    "tosses = [1] * n_heads + [0] * n_tails\n",
    "shuffle(tosses)\n",
    "\n",
    "print (tosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coins():\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.bar(list(Counter(tosses).keys()), list(Counter(tosses).values()))\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['tails', 'heads'])\n",
    "    ax.set_ylim(0, 20)\n",
    "    ax.set_yticks(np.arange(0, 21, 5))\n",
    "    return fig\n",
    "fig = plot_coins()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quesion:** Is this a fair coin (50% of chance generating heads)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Parameterize your problem using statistical distributions\n",
    "# Step 3. Write model in PyMC3, and justify your model structure\n",
    "\n",
    "# Context manager syntax. `coin_model` is **just** \n",
    "# a placeholder\n",
    "with pm.Model() as coin_model: \n",
    "    # Distributions are PyMC3 objects.\n",
    "    # Specify prior using Uniform object.\n",
    "    p_prior = ____\n",
    "    \n",
    "    # Specify likelihood using Bernoulli object.\n",
    "    like = ____  \n",
    "                        # \"observed=data\" is key\n",
    "                        # for likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Why not using Binomial distribution for the likelihood function?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Hit the Inference Button\n",
    "\n",
    "with coin_model:\n",
    "    # don't worry about this:\n",
    "    step = pm.Metropolis()\n",
    "    \n",
    "    # focus on this, the Inference Button:\n",
    "    coin_trace = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Interpret based on posterior distributions\n",
    "\n",
    "pm.traceplot(coin_trace)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(coin_trace[100:], color='#87ceeb', \n",
    "                  rope=[0.48, 0.52], point_estimate='mean', \n",
    "                  ref_val=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(coin_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 95% highest posterior density (HPD) encompasses the region of practical equivalence (ROPE).\n",
    "- What should we do? GET MORE DATA!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on:\n",
    "Modify the collected data as such: we have 60 tosses in total, and 22 of them are head. And then run all the cells again and see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 PyMC3 and Parameter Estimation <a id='32'>\n",
    "\n",
    "![bayes](../images/pymc3_logo.jpg)\n",
    "\n",
    "- Library of statistical distributions, sampling algorithms, and syntax for specifying statistical models\n",
    "- Everything in Python!\n",
    "\n",
    "### Common statistical analysis problems\n",
    "\n",
    "- Parameter estimation: \"is the true value equal to X?\"\n",
    "- Comparison between experimental groups: \"are the treatments different from the control(s)?\" \"is design A better than design B?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem type 1: parameter estimation\n",
    "\"is the true value equal to X?\"\n",
    "\n",
    "OR\n",
    "\n",
    "\"given the data, for the parameter of interest, what is the probability distribution over the possible values?\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Estimate the Proportion of Water\n",
    "\n",
    "#### How much of a planet is covered in water?\n",
    "Good news: you're an astronomer that just discovered a new planet. Bad news: your telescope has a small field of view and you can only see one tiny point on the planet at a time. More bad news: you're also a starving grad student and you can only take 5 measurements on your monthly stipend.\n",
    "\n",
    "**With 5 measurements what is your estimate for how much of the planet is covered in water?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. collecting data \n",
    "\n",
    "observations = [0,0,0,1,1]\n",
    "water_observations = sum(observations)\n",
    "total_observations = len(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the water coverage reaches 70% with 5 measurements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Parameterize your problem using statistical distributions\n",
    "# Step 3. Write model in PyMC3, and justify your model structure\n",
    "\n",
    "with pm.Model() as planet_model: \n",
    "    # Distributions are PyMC3 objects.\n",
    "    # Specify prior using Uniform object.\n",
    "    p_prior = ____ \n",
    "    \n",
    "    # Specify likelihood using Binomial object.\n",
    "    like = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quesion:** Why don't we use Bernoulli distribution this time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Hit the Inference Button\n",
    "\n",
    "with planet_model:\n",
    "    # don't worry about this:\n",
    "    step = pm.Metropolis()\n",
    "    \n",
    "    # focus on this, the Inference Button:\n",
    "    water_5obs_trace = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(water_5obs_trace)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(water_5obs_trace,\n",
    "                  rope=[0.68, 0.72], point_estimate='mean', \n",
    "                  ref_val=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert your inference data to az.InferenceData** using the methods az.from_pymc3 or az.from_stan. Then try a couple plots such as\n",
    "\n",
    "- az.plot_trace\n",
    "- az.plot_autocorr\n",
    "- az.plot_forest\n",
    "\n",
    "Try running a summary function such as\n",
    "\n",
    "- az.summary\n",
    "\n",
    "You can read more cool functions here: https://arviz-devs.github.io/arviz/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data = az.from_pymc3(water_5obs_trace)\n",
    "az.summary(inference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(inference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Completion time of a visual search task\n",
    "\n",
    "Visual search is a type of perceptual task requiring attention that typically involves an active scan of the visual environment for a particular object or feature (the target) among other objects or features (the distractors). Visual search can take place with or without eye movements. The ability to consciously locate an object or target amongst a complex array of stimuli has been extensively studied over the past 40 years. Practical examples of using visual search can be seen in everyday life, such as when one is picking out a product on a supermarket shelf, when animals are searching for food among piles of leaves, when trying to find a friend in a large crowd of people, or simply when playing visual search games such as **Where's Wally?**\n",
    "\n",
    "![visual_search](../images/visual_search.png)\n",
    "\n",
    "Image from _A User Study of Visual Search Performance with Interactive 2D and 3D\n",
    "Storyboards (Schoeffmann et al.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. collecting data (pretend we have it)\n",
    "\n",
    "np.random.seed(15)\n",
    "\n",
    "completion_time = np.random.normal(20,8,40)\n",
    "mu = np.mean(completion_time)\n",
    "sigma = np.std(completion_time)\n",
    "print (\"mean = \", mu)\n",
    "print (\"STD = \",sigma)\n",
    "\n",
    "x_s = np.linspace(5, 35, 100)\n",
    "plt.plot(x_s, st.norm.pdf(x_s, completion_time.mean(), completion_time.std(ddof=1)))\n",
    "plt.hist(completion_time,bins=15, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Parameterize your problem using statistical distributions\n",
    "# Step 3. Write model in PyMC3, and justify your model structure\n",
    "\n",
    "with pm.Model() as visual_search: \n",
    "    # Distributions are PyMC3 objects.\n",
    "    # Specify prior using Uniform object.\n",
    "    p_prior = ____\n",
    "    \n",
    "    # Specify likelihood using Normal object.\n",
    "    like = ____  \n",
    "                        # \"observed=data\" is key\n",
    "                        # for likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Hit the Inference Button\n",
    "\n",
    "with visual_search:\n",
    "    # don't worry about this:\n",
    "    step = pm.Metropolis()\n",
    "    \n",
    "    # focus on this, the Inference Button:\n",
    "    visual_search_trace = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Interpret based on posterior distributions\n",
    "\n",
    "pm.traceplot(visual_search_trace)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(visual_search_trace, \n",
    "                  color='#87ceeb', point_estimate='mean', \n",
    "                  rope=[17.9, 18.1], ref_val=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(visual_search_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  Comparison between A/B groups <a id=\"33\">\n",
    "\n",
    "\"are my experimental treatments different from my controls?\"  \n",
    "(Which is typically done by Student t-test in frequentist's analysis)\n",
    "\n",
    "### Example: The drug IQ problem\n",
    "\n",
    "does a drug treatment affect IQ scores?\n",
    "\n",
    "(documented in Kruschke, 2013, example from [PyMC3 documentation](https://docs.pymc.io/notebooks/BEST.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. collecting data \n",
    "\n",
    "drug = [  99.,  110.,  107.,  104.,  103.,  105.,  105.,  110.,   99.,\n",
    "        109.,  100.,  102.,  104.,  104.,  100.,  104.,  101.,  104.,\n",
    "        101.,  100.,  109.,  104.,  105.,  112.,   97.,  106.,  103.,\n",
    "        101.,  101.,  104.,   96.,  102.,  101.,  100.,   92.,  108.,\n",
    "         97.,  106.,   96.,   90.,  109.,  108.,  105.,  104.,  110.,\n",
    "         92.,  100.]\n",
    "\n",
    "placebo = [  95.,  105.,  103.,   99.,  104.,   98.,  103.,  104.,  102.,\n",
    "         91.,   97.,  101.,  100.,  113.,   98.,  102.,  100.,  105.,\n",
    "         97.,   94.,  104.,   92.,   98.,  105.,  106.,  101.,  106.,\n",
    "        105.,  101.,  105.,  102.,   95.,   91.,   99.,   96.,  102.,\n",
    "         94.,   93.,   99.,   99.,  113.,   96.]\n",
    "\n",
    "def ECDF(data):\n",
    "    x = np.sort(data)\n",
    "    y = np.cumsum(x) / np.sum(x)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def plot_drug():\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    x_drug, y_drug = ECDF(drug)\n",
    "    ax.plot(x_drug, y_drug, label='drug, n={0}'.format(len(drug)))\n",
    "    x_placebo, y_placebo = ECDF(placebo)\n",
    "    ax.plot(x_placebo, y_placebo, label='placebo, n={0}'.format(len(placebo)))\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('IQ Score')\n",
    "    ax.set_ylabel('Cumulative Frequency')\n",
    "    ax.hlines(0.5, ax.get_xlim()[0], ax.get_xlim()[1], linestyle='--')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a frequentist point of view, is there a \"statistically significant\" difference\n",
    "# between the two treatments?\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(drug, placebo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_drug()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = np.concatenate([drug, placebo])\n",
    "labels = ['drug'] * len(drug) + ['placebo'] * len(placebo)\n",
    "\n",
    "data = pd.DataFrame([y_vals, labels]).T\n",
    "data.columns = ['IQ', 'treatment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kruschke](../images/kruschke_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Parameterize your problem using statistical distributions\n",
    "# Step 3. Write model in PyMC3, and justify your model structure\n",
    "\n",
    "with pm.Model() as kruschke_model:\n",
    "    # Focus on the use of Distribution Objects.\n",
    "    # Linking Distribution Objects together is done by \n",
    "    # passing objects into other objects' parameters.\n",
    "    mu_drug = pm.Normal('mu_drug', mu=0, sd=100**2)\n",
    "    mu_placebo = pm.Normal('mu_placebo', mu=0, sd=100**2)\n",
    "    sigma_drug = pm.HalfCauchy('sigma_drug', beta=100)\n",
    "    sigma_placebo = pm.HalfCauchy('sigma_placebo', beta=100)\n",
    "    nu = pm.Exponential('nu', lam=1/29) + 1\n",
    "    \n",
    "    drug_like = pm.StudentT('drug', nu=nu, mu=mu_drug, \n",
    "                            sd=sigma_drug, observed=drug)\n",
    "    placebo_like = pm.StudentT('placebo', nu=nu, mu=mu_placebo, \n",
    "                               sd=sigma_placebo, observed=placebo)\n",
    "    diff_means = pm.Deterministic('diff_means', mu_drug - mu_placebo)\n",
    "    pooled_sd = pm.Deterministic('pooled_sd', \n",
    "                                 np.sqrt(np.power(sigma_drug, 2) + \n",
    "                                         np.power(sigma_placebo, 2) / 2))\n",
    "    effect_size = pm.Deterministic('effect_size', \n",
    "                                   diff_means / pooled_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with kruschke_model:\n",
    "    kruschke_trace = pm.sample(10000, step=pm.Metropolis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(kruschke_trace[2000:], \n",
    "             varnames=['mu_drug', 'mu_placebo'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(kruschke_trace[2000:], color='#87ceeb',\n",
    "            varnames=['mu_drug', 'mu_placebo', 'diff_means'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forestplot_line(ax, kind):\n",
    "    widths = {'median': 2.8, 'iqr': 2.0, 'hpd': 1.0}\n",
    "    assert kind in widths.keys(), f'line kind must be one of {widths.keys()}'\n",
    "    lines = []\n",
    "    for child in ax.get_children():\n",
    "        if isinstance(child, mpl.lines.Line2D) and np.allclose(child.get_lw(), widths[kind]):\n",
    "            lines.append(child)\n",
    "    return lines\n",
    "\n",
    "def adjust_forestplot_for_slides(ax):    \n",
    "    for line in get_forestplot_line(ax, kind='median'):\n",
    "        line.set_markersize(10)\n",
    "\n",
    "    for line in get_forestplot_line(ax, kind='iqr'):\n",
    "        line.set_linewidth(5)\n",
    "\n",
    "    for line in get_forestplot_line(ax, kind='hpd'):\n",
    "        line.set_linewidth(3)\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(kruschke_trace[2000:], \n",
    "          varnames=['mu_drug', 'mu_placebo'])\n",
    "ax = plt.gca()\n",
    "ax = adjust_forestplot_for_slides(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forest plot: 95% HPD (thin line), IQR (thicker line), and median (dot) of the posterior distribution on the same axes, allowing us to directly compare the treatment and control.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_effect_size(ax):\n",
    "    height = ax.get_ylim()[1] * 0.5\n",
    "    ax.hlines(height, 0, 0.2, 'red', lw=5)\n",
    "    ax.hlines(height, 0.2, 0.8, 'blue', lw=5)\n",
    "    ax.hlines(height, 0.8, ax.get_xlim()[1], 'green', lw=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pm.plot_posterior(kruschke_trace[2000:], \n",
    "                       varnames=['effect_size'],\n",
    "                       color='#87ceeb')\n",
    "overlay_effect_size(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Effect size (Cohen's d, none to small, medium, large) could be anywhere from essentially nothing to large (95% HPD [0.0, 0.77]).\n",
    "- The drug is likely inconsequential.\n",
    "\n",
    "For more examples of PyMC3, please refer to the [official document](https://docs.pymc.io/notebooks/getting_started.html).\n",
    "\n",
    "<a href=\"#outline\">Going back to outline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Recap what we've learned today: <a id=\"summary\">\n",
    "\n",
    "## Part 0: Frequentist vs Baysian\n",
    "- The fundamental differences between frequentist and Bayesian\n",
    "- The limitation of frequentists' analysis  \n",
    "- Probability as credibility points assigned in a region\n",
    "\n",
    "## Part 1: Probability simulation\n",
    "- Generating data using probability distribution (model) and MCMC sampling\n",
    "- Know the basic distributions\n",
    "\n",
    "## Part 2: Bayes' Theorem and Bayesian Inference\n",
    "- Conditional probability\n",
    "- Bayes' theorem\n",
    "- Bayesian inference\n",
    "\n",
    "## Part 3: Bayesian statistical analysis \n",
    "- Basic steps of doing Bayesian statistical analysis\n",
    "- Problem type #1: parameter estimation \n",
    "- Problem type #2: comparing A/B groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Takeaway: \n",
    "### Basic steps of Bayesian Statistical Analysis\n",
    "\n",
    "1. Collect data\n",
    "2. Parameterize your problem using statistical distributions\n",
    "3. Write model in PyMC3, and justify your model structure\n",
    "4. Hit the Inference Button\n",
    "5. Interpret based on posterior distributions\n",
    "6. (optional) with new information, modify model structure.\n",
    "\n",
    "\n",
    "![summary](../images/summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h2><img src=\"../images/warning.png\" width=\"20\", align=\"left\">&nbsp;Warning</h2>\n",
    "    <br>Note: PyMC3 is a great tool that does the math for lazy programmers. You can also easily find a bunch of instructions and tutorials of using PyMC3. However, this lecture does not simply want you know how to repeat the same steps only, but clearly know the foundation of Bayesian statistics.\n",
    "    <br><br>\n",
    "Lastly, feel free to ping me for any question about the assignment.<br>\n",
    "My email: yi-chi.liao@aalto.fi\n",
    "</div>\n",
    "\n",
    "<a href=\"#outline\">Going back to outline</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
